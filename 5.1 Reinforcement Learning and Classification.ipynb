{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3D82waLqItO"
      },
      "source": [
        "# Reinforcement Learning and Classification\n",
        "\n",
        "This assignment is about **sequential decision making** under uncertainty (reinforcement learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jEcC9NKqItQ"
      },
      "source": [
        "\n",
        "* To make things concrete, we will first focus on decision making under **no** uncertainity (questions 1 and 2), i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
        "\n",
        "* (optional) Next we will work through one type of reinforcement learning algorithm called Q-learning (question 3). Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.\n",
        "\n",
        "* Finally, in question 4 you will be asked to explain differences between reinforcement learning and supervised learning and in question 5 write about decision trees and random forests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGtknnUVqItP"
      },
      "source": [
        "## Primer\n",
        "### Decision Making\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
        "two parts. First, how do we learn about the world? This involves both the\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
        "currently know about the world, how should we decide what to do, taking into\n",
        "account future events and observations that may change our conclusions?\n",
        "Typically, this will involve creating long-term plans covering possible future\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
        "known to produce good results and experiment with something new is known\n",
        "as the **exploration-exploitation dilemma**.\n",
        "\n",
        "### The exploration-exploitation trade-off\n",
        "\n",
        "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
        "in which case you will regret going there. On the other hand, it could also be\n",
        "much better. What should you do?\n",
        "It all depends on how much information you have about either restaurant,\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
        "better food for the remaining time, while otherwise you will have missed only\n",
        "one good meal out of many, making the potential risk quite small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9WIePUCqItR"
      },
      "source": [
        "### Markov Decision Processes\n",
        "Markov Decision Processes (MDPs) provide a mathematical framework for modeling sequential decision making under uncertainty. An *agent* moves between *states* in a *state space* choosing *actions* that affects the transition probabilities between states, and the subsequent *rewards* recieved after a jump. This is then repeated a finite or infinite number of epochs. The objective, or the *solution* of the MDP, is to optimize the accumulated rewards of the process.\n",
        "\n",
        "Thus, an MDP consists of five parts:\n",
        "\n",
        "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n",
        "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n",
        "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n",
        "* Transition probabilities $p(s_{t+1}|s_t,a_t)$ for jumping from state $s_t$ to state $s_{t+1}$ after taking action $a_t$\n",
        "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ resulting from the chosen action and subsequent transition\n",
        "\n",
        "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions for each state. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state,\n",
        "\n",
        "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$\n",
        "\n",
        "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if we think all future rewards should count equally, we would use $\\gamma = 1$, while if we value near-future rewards higher than more distant rewards, we would use $\\gamma < 1$. The expected total *discounted* reward then becomes\n",
        "\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n",
        "\n",
        "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is, we want to find the policy where\n",
        "\n",
        "$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n",
        "\n",
        "To solve this we use a dynamic programming equation called the *Bellman equation*, given by\n",
        "\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n",
        "\n",
        "It can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n",
        "\n",
        "A real world example would be an inventory control system. The states could be the amount of items we have in stock, and the actions would be the amount of items to order at the end of each month. The discrete time would be each month and the reward would be the profit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiO_zpY7qItS"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUyGq4olqItS"
      },
      "source": [
        "The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n",
        "\n",
        "* The agent starts in state **S** (see table below)\n",
        "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west.\n",
        "* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n",
        "* When reaching **F**, the game ends (absorbing state).\n",
        "* The numbers in the boxes represent the rewards you receive when moving into that box.\n",
        "* Assume no discount in this model: $\\gamma = 1$\n",
        "    \n",
        "    \n",
        "| | | |\n",
        "|----------|----------|---------|\n",
        "|-1 |1|**F**|\n",
        "|0|-1|1|  \n",
        "|-1 |0|-1|  \n",
        "|**S**|-1|1|\n",
        "\n",
        "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n",
        "\n",
        "**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. For instance, NESW will make a circle.\n",
        "\n",
        "**1b)** What is the optimal policy (i.e., the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid.\n",
        "\n",
        "**1c)** What is expected total reward for the policy in 1a)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNXhC88FXyid"
      },
      "source": [
        "#Answers to Question 1\n",
        "**1a)**\n",
        "\n",
        "The optimal path is \"E E N N N\" and it is unique.\n",
        "\n",
        "**1b)**\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "**1c)**\n",
        "\n",
        "The reward for the policy is 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68eeXsci4Sy0"
      },
      "source": [
        "##Un-necessary algorithm for Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtE_LC1Ik0xe",
        "outputId": "0dee76be-1175-4ba1-bfe8-06ff285b6e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0 0\n"
          ]
        }
      ],
      "source": [
        "#Beginning of coding an algorithm for question 1. Later realised no code was \"needed\" for answering the quesitons.\n",
        "\n",
        "import random\n",
        "\n",
        "data = {\n",
        "    (0, 0): \"S\",\n",
        "    (0, 1): -1,\n",
        "    (0, 2): 0,\n",
        "    (0, 3): -1,\n",
        "    (1, 0): -1,\n",
        "    (1, 1): 0,\n",
        "    (1, 2): -1,\n",
        "    (1, 3): 1,\n",
        "    (2, 0): 1,\n",
        "    (2, 1): -1,\n",
        "    (2, 2): 1,\n",
        "    (2, 3): \"F\"\n",
        "}\n",
        "\n",
        "def validDirections(x, y):\n",
        "    res = []\n",
        "    if x > 0:\n",
        "        res.append('W')\n",
        "    if x < 2:\n",
        "        res.append('E')\n",
        "    if y > 0:\n",
        "        res.append('S')\n",
        "    if y < 3:\n",
        "        res.append('N')\n",
        "    return res\n",
        "\n",
        "def move(agent, dir):\n",
        "  if dir == 'N':\n",
        "    agent.y += 1\n",
        "  if dir == 'S':\n",
        "    agent.y -= 1\n",
        "  if dir == 'E':\n",
        "    agent.x += 1\n",
        "  if dir == 'W':\n",
        "    agent.x -= 1\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.points = 0\n",
        "\n",
        "agent = Agent(0, 0)\n",
        "\n",
        "while agent.x != 2 & agent.y !=3:\n",
        "  dir = validDirections(agent.x, agent.y)\n",
        "  movedir = random.choice(dir)\n",
        "  move(agent, movedir)\n",
        "  if agent.x == 2 and agent.y == 3:\n",
        "    print(f' WON {agent.points}')\n",
        "    break\n",
        "  if agent.x == 0 and agent.y == 0:\n",
        "    print('back to start')\n",
        "    break\n",
        "  else:\n",
        "    agent.points += data[agent.x, agent.y]\n",
        "\n",
        "print(agent.x, agent.y, agent.points)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkIk-k7qItT"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJTFDikEqItT"
      },
      "source": [
        "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZdhW0AZDoZv"
      },
      "source": [
        "The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n",
        "\n",
        "```\n",
        "epsilon is a small value, threshold\n",
        "for x from i to infinity\n",
        "do\n",
        "    for each state s\n",
        "    do\n",
        "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "    end\n",
        "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n",
        "        for each state s,\n",
        "        do\n",
        "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n",
        "            return π, V_k\n",
        "        end\n",
        "end\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz3UqgozqItU"
      },
      "source": [
        "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probability 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$.\n",
        "\n",
        "**Reward**:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|0|0|\n",
        "|0|10|0|  \n",
        "|0|0|0|  \n",
        "\n",
        "\n",
        "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|0|8|0|\n",
        "|8|2|8|  \n",
        "|0|8|0|  \n",
        "  \n",
        "**Iteration 2**:  \n",
        "  \n",
        "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \n",
        "Action **S**: 0  \n",
        "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
        "Action **W**: 0\n",
        "\n",
        "Hence any action between **E** and **N** would be best at this stage.\n",
        "\n",
        "Similarly for cell (1,0):\n",
        "\n",
        "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n",
        "\n",
        "Similar calculations for remaining cells give us:\n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|5.76|10.88|5.76|\n",
        "|10.88|8.12|10.88|  \n",
        "|5.76|10.88|5.76|  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3vIdFpuqItU"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n",
        "\n",
        "**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n",
        "\n",
        "**2c)** Describe your interpretation of the discount factor $\\gamma$. What would happen in the two extreme cases $\\gamma = 0$ and $\\gamma = 1$? Given some MDP, what would be important things to consider when deciding on which value of $\\gamma$ to use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmt2COcb5SUB",
        "outputId": "c119e29c-363e-40eb-c433-d993a7f07fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal policy for (0, 0) is (0, 1)\n",
            "Optimal policy for (0, 1) is (1, 1)\n",
            "Optimal policy for (0, 2) is (1, 2)\n",
            "Optimal policy for (1, 0) is (1, 1)\n",
            "Optimal policy for (1, 1) is (1, 2)\n",
            "Optimal policy for (1, 2) is (1, 1)\n",
            "Optimal policy for (2, 0) is (2, 1)\n",
            "Optimal policy for (2, 1) is (1, 1)\n",
            "Optimal policy for (2, 2) is (2, 1)\n",
            "Iteration 74\n",
            "45.6 51.94 45.6 \n",
            "51.94 48.04 51.94 \n",
            "45.6 51.94 45.6 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "reward = {\n",
        "    (0, 0): 0,\n",
        "    (0, 1): 0,\n",
        "    (0, 2): 0,\n",
        "    (1, 0): 0,\n",
        "    (1, 1): 10,\n",
        "    (1, 2): 0,\n",
        "    (2, 0): 0,\n",
        "    (2, 1): 0,\n",
        "    (2, 2): 0\n",
        "}\n",
        "\n",
        "expected = {\n",
        "    (0, 0): 0,\n",
        "    (0, 1): 0,\n",
        "    (0, 2): 0,\n",
        "    (1, 0): 0,\n",
        "    (1, 1): 0,\n",
        "    (1, 2): 0,\n",
        "    (2, 0): 0,\n",
        "    (2, 1): 0,\n",
        "    (2, 2): 0\n",
        "}\n",
        "\n",
        "\n",
        "def pointsNextState(pos):\n",
        "  res = 0\n",
        "  bestPos = (0,0)\n",
        "  for i in range(2):\n",
        "    for j in [-1, 1]:\n",
        "      if((pos[0] + j, pos[1]) in reward):\n",
        "        if(reward[pos[0] + j, pos[1]] >= res):\n",
        "          res = reward[pos[0] + j, pos[1]]\n",
        "          bestPos = (pos[0] + j, pos[1])\n",
        "      if((pos[0], pos[1] + j) in reward):\n",
        "        if(reward[pos[0], pos[1] + j] >= res):\n",
        "          res = reward[pos[0], pos[1] + j]\n",
        "          bestPos = (pos[0], pos[1] + j)\n",
        "\n",
        "  return {'point' : res,\n",
        "          'bestPos' : bestPos\n",
        "         }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lam = 0.9\n",
        "prob_a = 0.8\n",
        "prob_noa = 0.2\n",
        "eps = 0.005\n",
        "\n",
        "for i in range(100):\n",
        "  tempexp = {}\n",
        "\n",
        "  for j in reward:\n",
        "    next = pointsNextState(j)\n",
        "    nextPos = next['bestPos']\n",
        "    rewardNext = reward[nextPos]\n",
        "    expectedNext =  expected[nextPos]\n",
        "    rewardHere = reward[j]\n",
        "    expectedHere = expected[j]\n",
        "\n",
        "    res = prob_a * (rewardNext + lam * expectedNext) + prob_noa * (rewardHere + lam * expectedHere)\n",
        "\n",
        "    tempexp[j] = round(res, 2)\n",
        "\n",
        "  all_smallerthan_eps = all(tempexp[k] - expected[k] < eps for k in expected)\n",
        "\n",
        "  if all_smallerthan_eps:\n",
        "    for k in reward:\n",
        "      optimalpol = pointsNextState(k)\n",
        "      optimal = optimalpol['bestPos']\n",
        "      print(f'Optimal policy for {k} is {optimal}')\n",
        "    break\n",
        "  else:\n",
        "    expected = tempexp\n",
        "\n",
        "\n",
        "\n",
        "print(f'Iteration {i}')\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "      print(f\"{expected.get((i, j), 0):<4}\", end=\" \")\n",
        "  print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ux8XJAPbPJv"
      },
      "source": [
        "#Answers to Question 2\n",
        "\n",
        "**2b)**\n",
        "\n",
        "The value algorithm doesnt depend on V0, it will converge to an optimum no matter the V0. Our policies along the way to the optimum will differ but the end result will be the same. Since we continously update our V matrix, for each iteration it will converge closer to the optimum and for example if we have choosen a very high value for one point in V0, each iteration will decrease it until it has converged to the optimum.\n",
        "\n",
        "**2c)**\n",
        "\n",
        "If choose a discount factor of 1, the algorithm would continue endlessly since we value all future expected values equal to actual reward. If we choose a discount factor of 0 we would only do one iteration of the algorithm since we only value actual reward and dont care for the future potential expected value. Its the same as discounting cash flow in finance. We value the money now more than the potential money in the future because of inflation and interests etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9tL23YlqItU"
      },
      "source": [
        "## Reinforcement Learning (RL) (Theory for optional question 3)\n",
        "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n",
        "\n",
        "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n",
        "\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n",
        "\n",
        "The value function and the action-value function are directly related through\n",
        "\n",
        "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n",
        "\n",
        "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n",
        "\n",
        "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n",
        "\n",
        "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n",
        "\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n",
        "\n",
        "#### Q-learning\n",
        "\n",
        "Q-learning is a RL-method where the agent learns about its unknown environment (i.e., the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n",
        "\n",
        "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$\n",
        "\n",
        "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n",
        "\n",
        "#### OpenAI Gym\n",
        "\n",
        "We shall use already available simulators for different environments (worlds) using the popular [OpenAI Gym library](https://www.gymlibrary.dev/). It just implements different types of simulators including ATARI games. Although here we will only focus on simple ones, such as the **Chain enviroment** illustrated below.\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
        "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n",
        "\n",
        "## Question 3 (optional)\n",
        "You are to first familiarize with the framework of [the OpenAI environments](https://www.gymlibrary.dev/), and then implement the Q-learning algorithm for the <code>NChain-v0</code> enviroment depicted above, using default parameters and a learning rate of $\\gamma=0.95$. Report the final $Q^*$ table after convergence of the algorithm. For an example on how to do this, you can refer to the Q-learning of the **Frozen lake environment** (<code>q_learning_frozen_lake.ipynb</code>), uploaded on Canvas. *Hint*: start with a small learning rate.\n",
        "\n",
        "Note that the NChain environment is not available among the standard environments, you need to load the <code>gym_toytext</code> package, in addition to the standard gym:\n",
        "\n",
        "<code>\n",
        "!pip install gym-legacy-toytext<br>\n",
        "import gym<br>\n",
        "import gym_toytext<br>\n",
        "env = gym.make(\"NChain-v0\")<br>\n",
        "</code>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfKSybVI-UN1"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "**4a)** What is the importance of exploration in reinforcement learning? Explain with an example.\n",
        "\n",
        "**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_53w67dN7m6"
      },
      "source": [
        "## Answers to Question 4\n",
        "\n",
        "**4a)**\n",
        "One could say that exploration is essential to not get stuck in local optimas when there is better global optimas. Thus, it is always a balancing act between exploitation and exploration since you in most cases don't know if there is a better state to be found. This can be showcased in many different situations like:\n",
        "- Following the same study-technique because you know it works to pass the exam. Though there might be a better way to study which will generate a higher grade with the same effort. But you might not want to try in case it will not be a better technique for you and you'll fail the exam.\n",
        "- Cooking the same dish every day of the week since you now that it is tasty. Followingly, there might be a new receipe worth trying which can be even more tastier, but you're not sure if you'll manage to cook it properly, hence there is also a risk of it being tasteless.\n",
        "\n",
        "In more real-world issues you can emphasize some of the concepts touched upon in the lectures with more strategy-based issues:\n",
        "- **Exploiting all the time and never trying new moves, thus many actions and states remain unknown**:\n",
        "\n",
        "  Can be emphazised by imagining navigating a maze. If you find a way through, though a long and tough one, you might stick to it even though you know there is probably a shortcut, but the risk of getting stuck is scary enough to not try.\n",
        "-**Exploration is critical to find the optimal solution**:\n",
        "\n",
        "  Imagine trying to solve the Rubik's cube. You can get almost all the way, but in the last few steps you have to do exploration and mess up previous work to be able to solve it.\n",
        "-**Exploring all the time makes you unable to accumulate experience to improve**:\n",
        "\n",
        "  If finding out two-rights followed by two-lefts in the maze leads to a dead end, make sure to not do the same mistake twice.\n",
        "\n",
        "\n",
        "**4b)**\n",
        "Foremost, reinforcement learning does not require data to learn from to train the model. Instead it learns by interacting with the environment. Since supervised learning aims to determin a classification from the data, the feedback is more immediate than the feedback of reinforcement learning where the reward is delayed.\n",
        "\n",
        "In terms of the previous question about exploration-exploitation, this only exists in reinforcement learning and not in supervised learning since the model predicts according to a set rule.\n",
        "\n",
        "This leads to supervised learning being mor \"set\", and it is static in a sence that once the model has decided how to classify the data, it does not change unless we explicitly re-train the model to adjust the classification. However, the reinforcement learning is more adjustable, since we can fine-tune the policy making to maximize the outcome iteratively.\n",
        "\n",
        "Lastly, it can be faster to see how well the model performes in the case of supervised learning since we immideatly can get feedback on the accuracy of the model, whilst feedback from the reinforcement learning is often delayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1iFSvirqItV"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n",
        "\n",
        "From an exisiting dataset a decision tree can be built. Choosing the optimal questions to ask in the right order based on each questions gini impurity the \"optimal\" tree is built. The decision tree can then be used on new data to draw conclusions based on the old data. The random forest is based on decision trees but instead of creating the optimal tree we create various random trees. The new data then is used on each of the trees in the random forest and each result is added together to give a picture of what conclusion to draw.\n",
        "\n",
        "**5b)** State at least one advantage and one drawback with using random forests over decision trees.\n",
        "\n",
        "Advantages\n",
        "\n",
        "\n",
        "Random forests are known for their high accuracy and resilience, thanks to the many decision trees involved. In contrast, individual decision trees can be less accurate, performing well on the data they were trained on but struggling with new, unseen data\n",
        "\n",
        "\n",
        "Disadvantages\n",
        "\n",
        "Interpreting the model is more challenging compared to a decision tree, which offers a clear path to decision-making by following its branches.\n",
        "\n",
        "Random forests can be slower in generating predictions due to their multiple decision trees. Each time a prediction is made, all the trees in the forest need to provide a prediction for the same input, followed by a voting process. This entire procedure can be time-consuming. While a single decision tree will compute faster, but lack of accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yHCotQGqItV"
      },
      "source": [
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "123ec00200262563abc7db73a7df297e3839d21b30cef8aa24288688fdbde7de"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
